# Telegram-бот, который отвечает на вопросы, получая ответы от локально развернутой LLM

**Инструкция по установке и запуску:**

1. **Установите Ollama** ([официальный гайд](https://ollama.com/download)):
```bash
# Для Linux
curl -fsSL https://ollama.com/install.sh | sh
```

2. **Загрузите модель DeepSeek R1:**
```bash
ollama pull deepseek-r1:7b  # Выберите нужный размер (1.5b, 7b, 14b и т.д.)
```

3. **Запустите Ollama в фоне:**
```bash
ollama serve  # Для постоянной работы используйте systemd/screen
```

4. **Установите зависимости для бота:**
```bash
pip install python-telegram-bot requests
```

5. **Настройте Telegram-токен:**
```bash
export TELEGRAM_TOKEN="ВАШ_ТЕЛЕГРАМ_ТОКЕН"
```

6. **Запустите бота:**
```bash
python bot.py
```

---

**Ключевые особенности:**
- Полная локальная работа без облачных API
- История диалога на 3 последних сообщения
- Поддержка разных размеров модели
- Температура ответов 0.7 (настройте под свои нужды)

**Требования к железу:**
| Модель   | Минимум RAM | Рекомендуется |
|----------|-------------|---------------|
| DeepSeek 1.5B | 4 ГБ       | 8 ГБ          |
| DeepSeek 7B   | 8 ГБ       | 16 ГБ         |
| DeepSeek 14B  | 16 ГБ      | 32 ГБ         |

Для лучшей производительности используйте GPU с поддержкой CUDA и укажите в Ollama:
```bash
OLLAMA_NUM_GPU=1 ollama serve
```

